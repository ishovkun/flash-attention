# flash-attention

This is a fork of [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal).

I am rewriting their kernels and add robustnest / performance optimizations.

* The variable names follow the notations from the original [paper](https://arxiv.org/abs/2205.14135).
